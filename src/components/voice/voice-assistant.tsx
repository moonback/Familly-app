import { useEffect, useRef, useState } from 'react';
import { Mic, Square } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { detectIntent, DetectedIntent } from '@/lib/gemini';
import { useVoiceAssistantSettings } from '@/context/voice-assistant-context';
import { toast } from '@/hooks/use-toast';

interface VoiceAssistantProps {
  onIntent: (intent: DetectedIntent) => void;
}

interface SpeechRecognitionEvent extends Event {
  results: SpeechRecognitionResultList;
}

interface SpeechRecognitionResultList {
  [index: number]: SpeechRecognitionResult;
  length: number;
}

interface SpeechRecognitionResult {
  [index: number]: SpeechRecognitionAlternative;
  isFinal: boolean;
  length: number;
}

interface SpeechRecognitionAlternative {
  transcript: string;
  confidence: number;
}

interface SpeechRecognitionErrorEvent extends Event {
  error: string;
  message: string;
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  onresult: (event: SpeechRecognitionEvent) => void;
  onend: () => void;
  onstart: () => void;
  onerror: (event: SpeechRecognitionErrorEvent) => void;
  start: () => void;
  stop: () => void;
}

export const VoiceAssistant = ({ onIntent }: VoiceAssistantProps) => {
  const { enabled } = useVoiceAssistantSettings();
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const [listening, setListening] = useState(false);
  const [retryCount, setRetryCount] = useState(0);
  const MAX_RETRIES = 3;
  const [hasRequestedPermission, setHasRequestedPermission] = useState(false);
  const [isMicrophoneAvailable, setIsMicrophoneAvailable] = useState(false);
  const streamRef = useRef<MediaStream | null>(null);

  const speak = (text: string) => {
    console.log('üîä Synth√®se vocale:', text);
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = 'fr-FR';
    utterance.onstart = () => console.log('üé§ D√©but de la synth√®se vocale');
    utterance.onend = () => console.log('üé§ Fin de la synth√®se vocale');
    utterance.onerror = (error) => console.error('‚ùå Erreur de synth√®se vocale:', error);
    window.speechSynthesis.speak(utterance);
  };

  const checkMicrophoneAvailability = async () => {
    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const audioDevices = devices.filter(device => device.kind === 'audioinput');
      console.log('üé§ P√©riph√©riques audio disponibles:', audioDevices);
      return audioDevices.length > 0;
    } catch (error) {
      console.error('‚ùå Erreur lors de la v√©rification des p√©riph√©riques:', error);
      return false;
    }
  };

  const requestMicrophonePermission = async () => {
    try {
      console.log('üé§ Demande d\'acc√®s au microphone...');
      
      // V√©rifier d'abord si des microphones sont disponibles
      const hasMicrophone = await checkMicrophoneAvailability();
      if (!hasMicrophone) {
        toast({
          title: "Aucun microphone d√©tect√©",
          description: "Veuillez connecter un microphone et r√©essayer",
          variant: "destructive"
        });
        return false;
      }

      // Demander la permission
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      
      // Garder une r√©f√©rence au stream
      streamRef.current = stream;
      console.log('‚úÖ Permission microphone accord√©e');
      setHasRequestedPermission(true);
      setIsMicrophoneAvailable(true);
      return true;
    } catch (error) {
      console.error('‚ùå Erreur lors de la demande de permission:', error);
      setIsMicrophoneAvailable(false);
      toast({
        title: "Acc√®s au microphone refus√©",
        description: "Veuillez autoriser l'acc√®s au microphone dans les param√®tres de votre navigateur",
        variant: "destructive"
      });
      return false;
    }
  };

  const stopMicrophone = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
  };

  const handleError = (error: string, message: string) => {
    console.error('‚ùå Erreur de reconnaissance vocale:', error, message);
    
    if (error === 'audio-capture') {
      setIsMicrophoneAvailable(false);
      stopMicrophone();
      
      if (!hasRequestedPermission) {
        requestMicrophonePermission();
      } else {
        toast({
          title: "Probl√®me avec le microphone",
          description: "Veuillez v√©rifier que votre microphone est bien connect√© et autoris√©",
          variant: "destructive"
        });
      }
      setListening(false);
    } else if (error === 'no-speech') {
      if (retryCount < MAX_RETRIES) {
        setRetryCount(prev => prev + 1);
        toast({
          title: "Je n'ai pas entendu de voix",
          description: "Veuillez parler plus fort ou v√©rifier votre microphone",
          variant: "destructive"
        });
        setTimeout(() => {
          if (recognitionRef.current && isMicrophoneAvailable) {
            recognitionRef.current.start();
          }
        }, 1000);
      } else {
        setRetryCount(0);
        setListening(false);
        toast({
          title: "Trop de tentatives",
          description: "Je n'ai pas pu entendre votre voix. Veuillez r√©essayer.",
          variant: "destructive"
        });
      }
    } else {
      setListening(false);
      toast({
        title: "Erreur de reconnaissance vocale",
        description: "Une erreur est survenue. Veuillez r√©essayer.",
        variant: "destructive"
      });
    }
  };

  const getResponse = async (text: string) => {
    try {
      const apiKey = import.meta.env.VITE_GEMINI_API_KEY;
      if (!apiKey) {
        console.error('‚ùå Cl√© API Gemini manquante');
        throw new Error('Cl√© API Gemini manquante');
      }

      // R√©cup√©rer le prompt personnalis√©
      const systemPrompt = localStorage.getItem('voiceAssistantPrompt') || `Tu es un assistant vocal familial nomm√© "FamilleIA". R√©ponds de mani√®re naturelle et conversationnelle en fran√ßais.`;

      console.log('üîÑ Appel de l\'API Gemini...');
      const response = await fetch(
        `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`,
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            contents: [{
              parts: [{
                text: `${systemPrompt}

Utilisateur: ${text}
Assistant:`
              }]
            }],
            generationConfig: {
              temperature: 0.7,
              topK: 40,
              topP: 0.95,
              maxOutputTokens: 1024,
            }
          })
        }
      );

      if (!response.ok) {
        const errorText = await response.text();
        console.error('‚ùå R√©ponse API non-OK:', response.status, errorText);
        throw new Error(`Erreur API: ${response.status} - ${errorText}`);
      }

      const data = await response.json();
      console.log('‚úÖ R√©ponse API re√ßue:', data);
      
      const generatedText = data.candidates?.[0]?.content?.parts?.[0]?.text;
      if (!generatedText) {
        console.error('‚ùå Pas de texte g√©n√©r√© dans la r√©ponse:', data);
        return "D√©sol√©, je n'ai pas pu g√©n√©rer une r√©ponse.";
      }

      return generatedText;
    } catch (error) {
      console.error('‚ùå Erreur lors de la g√©n√©ration de r√©ponse:', error);
      return "D√©sol√©, j'ai rencontr√© une erreur lors de notre conversation.";
    }
  };

  useEffect(() => {
    console.log('üîÑ Initialisation de la reconnaissance vocale');
    const SpeechRecognition =
      (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    if (!SpeechRecognition) {
      console.error('‚ùå La reconnaissance vocale n\'est pas support√©e par ce navigateur');
      toast({
        title: "Navigateur non support√©",
        description: "Votre navigateur ne supporte pas la reconnaissance vocale",
        variant: "destructive"
      });
      return;
    }
    const recognition = new SpeechRecognition() as SpeechRecognition;
    recognition.lang = 'fr-FR';
    recognition.interimResults = false;
    recognition.continuous = false;

    recognition.onstart = () => {
      console.log('üé§ D√©but de l\'√©coute');
      setListening(true);
      toast({
        title: "√âcoute en cours",
        description: "Je vous √©coute...",
      });
    };

    recognition.onend = () => {
      console.log('üé§ Fin de l\'√©coute');
      setListening(false);
    };

    recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
      handleError(event.error, event.message);
    };

    recognition.onresult = async (e: SpeechRecognitionEvent) => {
      const transcript = Array.from(e.results)
        .map((r) => r[0].transcript)
        .join(' ');
      console.log('üé§ Texte reconnu:', transcript);
      setRetryCount(0);

      // Obtenir une r√©ponse conversationnelle
      const response = await getResponse(transcript);
      speak(response);
    };

    recognitionRef.current = recognition;
    console.log('‚úÖ Reconnaissance vocale initialis√©e');

    // Nettoyage lors du d√©montage du composant
    return () => {
      stopMicrophone();
    };
  }, []);

  const toggle = async () => {
    if (!recognitionRef.current) {
      console.error('‚ùå La reconnaissance vocale n\'est pas initialis√©e');
      return;
    }

    if (!hasRequestedPermission || !isMicrophoneAvailable) {
      const permissionGranted = await requestMicrophonePermission();
      if (!permissionGranted) return;
    }

    if (listening) {
      console.log('üõë Arr√™t de l\'√©coute');
      recognitionRef.current.stop();
      setRetryCount(0);
    } else {
      console.log('‚ñ∂Ô∏è D√©marrage de l\'√©coute');
      setRetryCount(0);
      recognitionRef.current.start();
    }
  };

  if (!enabled) {
    console.log('üîá Assistant vocal d√©sactiv√©');
    return null;
  }

  return (
    <Button variant="outline" size="icon" onClick={toggle} className="ml-2">
      {listening ? <Square className="h-4 w-4" /> : <Mic className="h-4 w-4" />}
    </Button>
  );
};
